{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Learn the Basics <intro.html>`_ ||\n",
    "`Quickstart <quickstart_tutorial.html>`_ ||\n",
    "`Tensors <tensorqs_tutorial.html>`_ ||\n",
    "`Datasets & DataLoaders <data_tutorial.html>`_ ||\n",
    "`Transforms <transforms_tutorial.html>`_ ||\n",
    "**Build Model** ||\n",
    "`Autograd <autogradqs_tutorial.html>`_ ||\n",
    "`Optimization <optimization_tutorial.html>`_ ||\n",
    "`Save & Load Model <saveloadrun_tutorial.html>`_\n",
    "\n",
    "Build the Neural Network\n",
    "===================\n",
    "\n",
    "Neural networks comprise of layers/modules that perform operations on data.\n",
    "The `torch.nn <https://pytorch.org/docs/stable/nn.html>`_ namespace provides all the building blocks you need to\n",
    "build your own neural network. Every module in PyTorch subclasses the `nn.Module <https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_.\n",
    "A neural network is a module itself that consists of other modules (layers). This nested structure allows for\n",
    "building and managing complex architectures easily.\n",
    "\n",
    "In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Device for Training\n",
    "-----------------------\n",
    "We want to be able to train our model on a hardware accelerator like the GPU,\n",
    "if it is available. Let's check to see if\n",
    "`torch.cuda <https://pytorch.org/docs/stable/notes/cuda.html>`_ is available, else we\n",
    "continue to use the CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dml device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"dml\" if torch.has_dml else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Class\n",
    "-------------------------\n",
    "We define our neural network by subclassing ``nn.Module``, and\n",
    "initialize the neural network layers in ``__init__``. Every ``nn.Module`` subclass implements\n",
    "the operations on input data in the ``forward`` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of ``NeuralNetwork``, and move it to the ``device``, and print\n",
    "its structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model's ``forward``,\n",
    "along with some `background operations <https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866>`_.\n",
    "Do not call ``model.forward()`` directly!\n",
    "\n",
    "Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class.\n",
    "We get the prediction probabilities by passing it through an instance of the ``nn.Softmax`` module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not run 'aten::_softmax' with arguments from the 'UNKNOWN_TENSOR_TYPE_ID' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_softmax' is only available for these backends: [CPU, MkldnnCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\build\\aten\\src\\ATen\\RegisterCPU.cpp:5926 [kernel]\nMkldnnCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\build\\aten\\src\\ATen\\RegisterMkldnnCPU.cpp:285 [kernel]\nBackendSelect: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradCUDA: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradXLA: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradNestedTensor: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse1: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse2: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse3: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nTracer: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\TraceType_0.cpp:10499 [kernel]\nAutocast: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\autocast_mode.cpp:250 [backend fallback]\nBatched: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16680\\4205252085.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mX\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrand\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m28\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m28\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mlogits\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mpred_probab\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSoftmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlogits\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpred_probab\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Predicted class: {y_pred}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m   1198\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1199\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1200\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msoftmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_stacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1201\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1202\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36msoftmax\u001B[1;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[0;32m   1581\u001B[0m         \u001B[0mdim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_softmax_dim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"softmax\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_stacklevel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1582\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1583\u001B[1;33m         \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msoftmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1584\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1585\u001B[0m         \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msoftmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Could not run 'aten::_softmax' with arguments from the 'UNKNOWN_TENSOR_TYPE_ID' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_softmax' is only available for these backends: [CPU, MkldnnCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\build\\aten\\src\\ATen\\RegisterCPU.cpp:5926 [kernel]\nMkldnnCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\build\\aten\\src\\ATen\\RegisterMkldnnCPU.cpp:285 [kernel]\nBackendSelect: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradCUDA: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradXLA: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradNestedTensor: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse1: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse2: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse3: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:9273 [autograd kernel]\nTracer: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\TraceType_0.cpp:10499 [kernel]\nAutocast: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\autocast_mode.cpp:250 [backend fallback]\nBatched: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Layers\n-------------------------\n\nLet's break down the layers in the FashionMNIST model. To illustrate it, we\nwill take a sample minibatch of 3 images of size 28x28 and see what happens to it as\nwe pass it through the network.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\nprint(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Flatten\n^^^^^^^^^^^^^^^^^^^^^^\nWe initialize the `nn.Flatten  <https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html>`_\nlayer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\nthe minibatch dimension (at dim=0) is maintained).\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\nflat_image = flatten(input_image)\nprint(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear\n^^^^^^^^^^^^^^^^^^^^^^\nThe `linear layer <https://pytorch.org/docs/stable/generated/torch.nn.Linear.html>`_\nis a module that applies a linear transformation on the input using its stored weights and biases.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\nhidden1 = layer1(flat_image)\nprint(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.ReLU\n^^^^^^^^^^^^^^^^^^^^^^\nNon-linear activations are what create the complex mappings between the model's inputs and outputs.\nThey are applied after linear transformations to introduce *nonlinearity*, helping neural networks\nlearn a wide variety of phenomena.\n\nIn this model, we use `nn.ReLU <https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html>`_ between our\nlinear layers, but there's other activations to introduce non-linearity in your model.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.4053,  0.1567, -0.2813, -0.2896,  0.0811, -0.2975, -0.0588,  0.3808,\n",
      "         -0.0887,  1.0351, -0.0432, -0.2054,  0.2390,  0.2783,  0.8173, -0.4089,\n",
      "         -0.1321, -0.1031,  0.0847,  0.4045],\n",
      "        [ 0.3224,  0.5597, -0.4253, -0.6166, -0.0310, -0.2892, -0.0506,  0.3274,\n",
      "         -0.0572,  0.9874, -0.2365, -0.4003,  0.0596,  0.1764,  0.8063, -0.3583,\n",
      "          0.0181, -0.5551,  0.1829,  0.3007],\n",
      "        [ 0.2793,  0.2145, -0.1999, -0.3787,  0.0015, -0.0086, -0.0063,  0.5251,\n",
      "          0.1464,  0.7235, -0.2387, -0.2712,  0.3715,  0.1161,  0.8008, -0.6308,\n",
      "          0.2388, -0.6009,  0.0188,  0.2716]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.4053, 0.1567, 0.0000, 0.0000, 0.0811, 0.0000, 0.0000, 0.3808, 0.0000,\n",
      "         1.0351, 0.0000, 0.0000, 0.2390, 0.2783, 0.8173, 0.0000, 0.0000, 0.0000,\n",
      "         0.0847, 0.4045],\n",
      "        [0.3224, 0.5597, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3274, 0.0000,\n",
      "         0.9874, 0.0000, 0.0000, 0.0596, 0.1764, 0.8063, 0.0000, 0.0181, 0.0000,\n",
      "         0.1829, 0.3007],\n",
      "        [0.2793, 0.2145, 0.0000, 0.0000, 0.0015, 0.0000, 0.0000, 0.5251, 0.1464,\n",
      "         0.7235, 0.0000, 0.0000, 0.3715, 0.1161, 0.8008, 0.0000, 0.2388, 0.0000,\n",
      "         0.0188, 0.2716]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\nhidden1 = nn.ReLU()(hidden1)\nprint(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential\n^^^^^^^^^^^^^^^^^^^^^^\n`nn.Sequential <https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html>`_ is an ordered\ncontainer of modules. The data is passed through all the modules in the same order as defined. You can use\nsequential containers to put together a quick network like ``seq_modules``.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n    flatten,\n    layer1,\n    nn.ReLU(),\n    nn.Linear(20, 10)\n)\ninput_image = torch.rand(3,28,28)\nlogits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Softmax\n^^^^^^^^^^^^^^^^^^^^^^\nThe last linear layer of the neural network returns `logits` - raw values in [-\\infty, \\infty] - which are passed to the\n`nn.Softmax <https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html>`_ module. The logits are scaled to values\n[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\nwhich the values must sum to 1.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\npred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters\n-------------------------\nMany layers inside a neural network are *parameterized*, i.e. have associated weights\nand biases that are optimized during training. Subclassing ``nn.Module`` automatically\ntracks all fields defined inside your model object, and makes all parameters\naccessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n\nIn this example, we iterate over each parameter, and print its size and a preview of its values.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not run 'aten::masked_select' with arguments from the 'UNKNOWN_TENSOR_TYPE_ID' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::masked_select' is only available for these backends: [CPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\build\\aten\\src\\ATen\\RegisterCPU.cpp:5926 [kernel]\nBackendSelect: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nAutogradOther: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradCUDA: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradXLA: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradNestedTensor: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradPrivateUse1: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradPrivateUse2: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradPrivateUse3: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nTracer: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\TraceType_4.cpp:10612 [kernel]\nAutocast: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\autocast_mode.cpp:250 [backend fallback]\nBatched: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16680\\1994039549.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparam\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnamed_parameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\miniconda3\\envs\\py37\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36m__format__\u001B[1;34m(self, format_spec)\u001B[0m\n\u001B[0;32m    543\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    544\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__format__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mformat_spec\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 545\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__format__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mformat_spec\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    546\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    547\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__ipow__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mother\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# type: ignore[misc]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\py37\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36m__repr__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    191\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__repr__\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    192\u001B[0m         \u001B[1;31m# All strings are unicode in Python 3.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 193\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tensor_str\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    194\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\py37\\lib\\site-packages\\torch\\_tensor_str.py\u001B[0m in \u001B[0;36m_str\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    381\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    382\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 383\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_str_intern\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\miniconda3\\envs\\py37\\lib\\site-packages\\torch\\_tensor_str.py\u001B[0m in \u001B[0;36m_str_intern\u001B[1;34m(inp)\u001B[0m\n\u001B[0;32m    356\u001B[0m                     \u001B[0mtensor_str\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_tensor_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_dense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindent\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    357\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 358\u001B[1;33m                     \u001B[0mtensor_str\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_tensor_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindent\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    359\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    360\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayout\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrided\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\py37\\lib\\site-packages\\torch\\_tensor_str.py\u001B[0m in \u001B[0;36m_tensor_str\u001B[1;34m(self, indent)\u001B[0m\n\u001B[0;32m    240\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_tensor_str_with_formatter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msummarize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreal_formatter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimag_formatter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    241\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 242\u001B[1;33m         \u001B[0mformatter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_Formatter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_summarized_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0msummarize\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    243\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_tensor_str_with_formatter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msummarize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mformatter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    244\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\py37\\lib\\site-packages\\torch\\_tensor_str.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, tensor)\u001B[0m\n\u001B[0;32m     88\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 90\u001B[1;33m             \u001B[0mnonzero_finite_vals\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmasked_select\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor_view\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfinite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor_view\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m&\u001B[0m \u001B[0mtensor_view\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mne\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     91\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mnonzero_finite_vals\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Could not run 'aten::masked_select' with arguments from the 'UNKNOWN_TENSOR_TYPE_ID' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::masked_select' is only available for these backends: [CPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\build\\aten\\src\\ATen\\RegisterCPU.cpp:5926 [kernel]\nBackendSelect: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nAutogradOther: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradCPU: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradCUDA: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradXLA: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradNestedTensor: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradPrivateUse1: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradPrivateUse2: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nAutogradPrivateUse3: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:8893 [autograd kernel]\nTracer: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\torch\\csrc\\autograd\\generated\\TraceType_4.cpp:10612 [kernel]\nAutocast: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\autocast_mode.cpp:250 [backend fallback]\nBatched: registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at D:\\a\\_work\\1\\s\\pytorch-directml\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n\nfor name, param in model.named_parameters():\n    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading\n--------------\n- `torch.nn API <https://pytorch.org/docs/stable/nn.html>`_\n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}